{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train macro & fundamental-aware price models \n",
    "Pretraining with fundamental, macroeconomic, estimate and sharep price data to capture the data patterns.\n",
    "Use embedded fundamental/macro/short-term information for return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import FundamentalDataset, PriceDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "from utils import Defaults\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "DEFAULTS = Defaults\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train autoencoders as pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Train encoders on fundamental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def expand_mask(mask: torch.tensor, target_dim: int) -> torch.tensor:\n",
    "    \"\"\"expand mask from n dimensions to n+1 dimensions\"\"\"\n",
    "    newmask = deepcopy(mask).unsqueeze(-1)\n",
    "    mask_dims = list(newmask.shape)\n",
    "    mask_dims[-1] = target_dim\n",
    "    mask_dims = tuple(mask_dims)\n",
    "    return newmask.expand(mask_dims)\n",
    "\n",
    "def expand_masks(masks: Sequence[torch.tensor], target_dims: Sequence[int]):\n",
    "    expanded_masks = []\n",
    "    for mask, dim in zip(masks, target_dims):\n",
    "        newmask = expand_mask(mask, dim)\n",
    "        expanded_masks.append(newmask)\n",
    "    return expanded_masks\n",
    "\n",
    "def masked_mse_loss(\n",
    "        input: torch.tensor, \n",
    "        target: torch.tensor,\n",
    "        mask: torch.tensor,\n",
    "        na_pad: torch.tensor,\n",
    "        ) -> torch.tensor:\n",
    "    \"\"\"custome MSE loss to mask padding & nan values\n",
    "    :param input: original vector\n",
    "    :param target: target vector\n",
    "    :param \n",
    "    \"\"\"\n",
    "    loss = nn.MSELoss()\n",
    "    dims = input.shape[-1]\n",
    "    na_mask = input == na_pad\n",
    "    expanded_mask = expand_mask(mask, dims)\n",
    "    new_mask = na_mask.type(torch.bool) + expanded_mask.type(torch.bool)\n",
    "    masked_input = torch.masked_select(input, ~new_mask) # mask itmodel is True if masked\n",
    "    masked_target = torch.masked_select(target, ~new_mask)\n",
    "    loss = loss(masked_input, masked_target)\n",
    "    input_size = (masked_input == masked_input).sum()\n",
    "    return loss / input_size # normalize input size\n",
    "\n",
    "def composite_mseloss(mse_losses: Sequence[torch.tensor]):\n",
    "    mean_loss = torch.stack(mse_losses).sum() / len(mse_losses)\n",
    "    penalty_loss = torch.stack([(loss - mean_loss)**2 for loss in mse_losses]).sum()\n",
    "    composite_loss = mean_loss + penalty_loss\n",
    "    return composite_loss\n",
    "\n",
    "def multiple_input_masked_mse_loss(\n",
    "        inputs: Sequence[torch.tensor],\n",
    "        targets: Sequence[torch.tensor],\n",
    "        masks: Sequence[torch.tensor],\n",
    "        na_pads: Sequence[torch.tensor]):\n",
    "    losses = []\n",
    "    for input, target, mask, na_pad in zip(\n",
    "        inputs, targets, masks, na_pads):\n",
    "        loss = masked_mse_loss(input, target, mask, na_pad)\n",
    "        losses.append(loss)\n",
    "    composite_loss = composite_mseloss(losses)\n",
    "    return composite_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "def encode(\n",
    "        model, \n",
    "        inputs: Sequence[Tuple[torch.tensor, Optional[torch.tensor]]],\n",
    "        padding_masks: Sequence[torch.tensor]\n",
    "        ) -> Tuple[torch.tensor, Tuple[torch.tensor]]:\n",
    "    \"\"\"encode a list of inputs of different lengths and dimensionalities\n",
    "    into a single embedding vector\n",
    "    \"\"\"\n",
    "    embeddings, memories = [], []\n",
    "    for input, mask, transformer_encoder, linear_encoder in zip(\n",
    "        inputs, padding_masks, model.transformer_encoders, model.linear_encoder_layers):\n",
    "        x_ = transformer_encoder(input, src_key_padding_mask=mask)\n",
    "        print(x_.shape)\n",
    "        memories.append(x_)\n",
    "        embedded = linear_encoder(x_)\n",
    "        print(embedded.shape)\n",
    "        embeddings.append(embedded)\n",
    "    _embedding = torch.stack(embeddings, dim=-1)\n",
    "    embedding = model.linear_encoder(_embedding)\n",
    "    embedding = model.tanh(embedding)\n",
    "    return (embedding, memories)\n",
    "\n",
    "def decode(\n",
    "        model, \n",
    "        embedding: torch.tensor, \n",
    "        memories: Sequence[torch.tensor]) -> Sequence[torch.tensor]:\n",
    "    _embeddings = model.linear_decoder(embedding)\n",
    "    reconstructed_xs = []\n",
    "    for i in range(model.num_inputs):\n",
    "        _output = model.linear_decoder_layers[i](_embeddings[:, :, i])\n",
    "        output = model.transformer_decoders[i](_output, memory=memories[i])\n",
    "        reconstructed_xs.append(output)\n",
    "    return reconstructed_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 128 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught DBAPIError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1960, in _exec_single_context\n    self.dialect.do_execute(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 924, in do_execute\n    cursor.execute(statement, parameters)\npyodbc.Error: ('HY000', \"[HY000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Resource ID : 1. The request limit for the database is 75 and has been reached. See 'https://docs.microsoft.com/azure/azure-sql/database/resource-limits-logical-server' for assistance. (10928) (SQLExecDirectW)\")\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/timeseries-t4/code/users/p.peng/stockencoder/data/__init__.py\", line 364, in __getitem__\n    slice = SQLDatabase.to_pandas(query).astype(float).ffill()\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/timeseries-t4/code/users/p.peng/stockencoder/utils/database.py\", line 59, in to_pandas\n    return cls(**kwargs).query_to_pandas(query)\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/timeseries-t4/code/users/p.peng/stockencoder/utils/database.py\", line 54, in query_to_pandas\n    df = pd.read_sql(sqlalchemy.text(query), con=conn)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/pandas/io/sql.py\", line 734, in read_sql\n    return pandas_sql.read_query(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/pandas/io/sql.py\", line 1836, in read_query\n    result = self.execute(sql, params)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/pandas/io/sql.py\", line 1660, in execute\n    return self.con.execute(sql, *args)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1408, in execute\n    return meth(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 513, in _execute_on_connection\n    return connection._execute_clauseelement(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1630, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1839, in _execute_context\n    return self._exec_single_context(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1979, in _exec_single_context\n    self._handle_dbapi_exception(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2335, in _handle_dbapi_exception\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1960, in _exec_single_context\n    self.dialect.do_execute(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 924, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY000', \"[HY000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Resource ID : 1. The request limit for the database is 75 and has been reached. See 'https://docs.microsoft.com/azure/azure-sql/database/resource-limits-logical-server' for assistance. (10928) (SQLExecDirectW)\")\n[SQL: SELECT operating_roic, normalized_roe, return_on_asset, return_com_eqy, ebit_margin, fcf_margin_after_oper_lea_pymt, gross_margin, eff_tax_rate, ebitda_margin, net_debt_to_shrhldr_eqty, fixed_charge_coverage_ratio, net_debt_to_ebitda, acct_rcv_days, cash_conversion_cycle, invent_days, net_income_growth, sales_rev_turn_growth FROM fundamental_data_stock_encoder WHERE (year between 2003 AND 2012) AND (figi = 'BBG000BDZJK6')]\n(Background on this error at: https://sqlalche.me/e/20/dbapi)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPOCHS):\n\u001b[1;32m     57\u001b[0m     running_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fundamental_data_loader):\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m     60\u001b[0m         pct_nan \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m==\u001b[39m DEFAULTS\u001b[38;5;241m.\u001b[39mpadding_val)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mdetach() \n",
      "File \u001b[0;32m/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/_utils.py:721\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(msg)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught DBAPIError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1960, in _exec_single_context\n    self.dialect.do_execute(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 924, in do_execute\n    cursor.execute(statement, parameters)\npyodbc.Error: ('HY000', \"[HY000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Resource ID : 1. The request limit for the database is 75 and has been reached. See 'https://docs.microsoft.com/azure/azure-sql/database/resource-limits-logical-server' for assistance. (10928) (SQLExecDirectW)\")\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/timeseries-t4/code/users/p.peng/stockencoder/data/__init__.py\", line 364, in __getitem__\n    slice = SQLDatabase.to_pandas(query).astype(float).ffill()\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/timeseries-t4/code/users/p.peng/stockencoder/utils/database.py\", line 59, in to_pandas\n    return cls(**kwargs).query_to_pandas(query)\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/timeseries-t4/code/users/p.peng/stockencoder/utils/database.py\", line 54, in query_to_pandas\n    df = pd.read_sql(sqlalchemy.text(query), con=conn)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/pandas/io/sql.py\", line 734, in read_sql\n    return pandas_sql.read_query(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/pandas/io/sql.py\", line 1836, in read_query\n    result = self.execute(sql, params)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/pandas/io/sql.py\", line 1660, in execute\n    return self.con.execute(sql, *args)\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1408, in execute\n    return meth(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 513, in _execute_on_connection\n    return connection._execute_clauseelement(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1630, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1839, in _execute_context\n    return self._exec_single_context(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1979, in _exec_single_context\n    self._handle_dbapi_exception(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2335, in _handle_dbapi_exception\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1960, in _exec_single_context\n    self.dialect.do_execute(\n  File \"/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 924, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY000', \"[HY000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Resource ID : 1. The request limit for the database is 75 and has been reached. See 'https://docs.microsoft.com/azure/azure-sql/database/resource-limits-logical-server' for assistance. (10928) (SQLExecDirectW)\")\n[SQL: SELECT operating_roic, normalized_roe, return_on_asset, return_com_eqy, ebit_margin, fcf_margin_after_oper_lea_pymt, gross_margin, eff_tax_rate, ebitda_margin, net_debt_to_shrhldr_eqty, fixed_charge_coverage_ratio, net_debt_to_ebitda, acct_rcv_days, cash_conversion_cycle, invent_days, net_income_growth, sales_rev_turn_growth FROM fundamental_data_stock_encoder WHERE (year between 2003 AND 2012) AND (figi = 'BBG000BDZJK6')]\n(Background on this error at: https://sqlalche.me/e/20/dbapi)\n"
     ]
    }
   ],
   "source": [
    "from models.autoencoder import BaseAutoEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "RUN = 5\n",
    "LR = 1e-9\n",
    "NUM_TRANSFORMER_LAYERS = 1\n",
    "WINDOW_SIZE = 10\n",
    "NHEADS = 1\n",
    "ENCODING_DIM = 2\n",
    "MAX_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "PCT_NAN_THRES = 0.4\n",
    "\n",
    "fund_data = FundamentalDataset(window_size=WINDOW_SIZE)\n",
    "# fund_data_weekly = FundamentalDataset(freq=\"W\")\n",
    "price_data = PriceDataset()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data_ls, masks = [], []\n",
    "    for data, mask in batch:\n",
    "        data_ls.append(data)\n",
    "        masks.append(mask)\n",
    "    return (\n",
    "        torch.stack(data_ls),\n",
    "        torch.stack(masks)\n",
    "    )\n",
    "\n",
    "logger_stem = \"./traininglog/fundamental_encoder/runs/\"\n",
    "logger = SummaryWriter(f\"{logger_stem}run{RUN};lr={LR};notflayrs={NUM_TRANSFORMER_LAYERS};wd={WINDOW_SIZE};nh={NHEADS};edim={ENCODING_DIM};bsize={BATCH_SIZE}\")\n",
    "\n",
    "fundamental_data_loader = DataLoader(\n",
    "    fund_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=max(BATCH_SIZE, 8) # max 8 workers suggested\n",
    ")\n",
    "\n",
    "num_batches = len(fundamental_data_loader)\n",
    "\n",
    "model = BaseAutoEncoder(\n",
    "    window_sizes=[WINDOW_SIZE],\n",
    "    encoding_dim=ENCODING_DIM, \n",
    "    num_transformer_layers=[NUM_TRANSFORMER_LAYERS], \n",
    "    dims=[17],\n",
    "    activation_func=F.tanh,\n",
    "    nheads=[NHEADS],\n",
    "    device=DEVICE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LR, betas=[0.9, 0.99], eps=1e-09)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    running_losses = []\n",
    "    for i, (input, mask) in enumerate(fundamental_data_loader):\n",
    "        print(i)\n",
    "        pct_nan = (input == DEFAULTS.padding_val).sum() / (input == input).sum().detach() \n",
    "        if pct_nan < PCT_NAN_THRES: # skip if a lot of nan\n",
    "            # forward pass\n",
    "            input, mask = input.to(DEVICE), mask.to(DEVICE)\n",
    "            embedding, memories = model.encode([input], [mask])\n",
    "            output = model.decode(embedding, memories)\n",
    "            loss = multiple_input_masked_mse_loss([input], output, [mask], [DEFAULTS.padding_val])\n",
    "            running_losses.append(loss.item())\n",
    "            logger.add_scalar(\"loss/train_step\", loss.item(), epoch*num_batches + i)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    mean_loss = np.mean(running_losses)\n",
    "    logger.add_scalar(\"loss/train\", mean_loss, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p.peng\\Anaconda3\\envs\\stockencoder_env\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:366: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e+10,  2.2513e-01, -1.0000e+10,  8.9167e-01,  9.5669e-01],\n",
       "        [-1.0000e+10,  2.2591e-01, -1.0000e+10,  8.9041e-01,  9.5619e-01],\n",
       "        [-1.0000e+10,  2.2721e-01, -1.0000e+10,  8.8854e-01,  9.5544e-01],\n",
       "        ...,\n",
       "        [-1.0000e+10,  2.7621e-01, -6.6862e+00,  8.8338e-01,  9.2140e-01],\n",
       "        [-1.0000e+10,  2.7616e-01, -2.5919e+00,  8.8354e-01,  9.2150e-01],\n",
       "        [-1.0000e+10,  2.7332e-01, -1.0000e+10,  8.9210e-01,  9.2715e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data[(1, dt.date(2022, 1, 1))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
