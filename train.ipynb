{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train macro & fundamental-aware price models \n",
    "Pretraining with fundamental, macroeconomic, estimate and sharep price data to capture the data patterns.\n",
    "Use embedded fundamental/macro/short-term information for return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import FundamentalDataset, PriceDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "from utils import Defaults\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "DEFAULTS = Defaults\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "fund_data = FundamentalDataset()\n",
    "fund_data_weekly = FundamentalDataset(freq=\"W\")\n",
    "price_data = PriceDataset()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data_ls, masks = [], []\n",
    "    for data, mask in batch:\n",
    "        data_ls.append(data)\n",
    "        masks.append(mask)\n",
    "    return (\n",
    "        torch.stack(data_ls),\n",
    "        torch.stack(masks)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train autoencoders as pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Train encoders on fundamental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def expand_mask(mask: torch.tensor, target_dim: int) -> torch.tensor:\n",
    "    \"\"\"expand mask from n dimensions to n+1 dimensions\"\"\"\n",
    "    newmask = deepcopy(mask).unsqueeze(-1)\n",
    "    mask_dims = list(newmask.shape)\n",
    "    mask_dims[-1] = target_dim\n",
    "    mask_dims = tuple(mask_dims)\n",
    "    return newmask.expand(mask_dims)\n",
    "\n",
    "def expand_masks(masks: Sequence[torch.tensor], target_dims: Sequence[int]):\n",
    "    expanded_masks = []\n",
    "    for mask, dim in zip(masks, target_dims):\n",
    "        newmask = expand_mask(mask, dim)\n",
    "        expanded_masks.append(newmask)\n",
    "    return expanded_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(\n",
    "        input: torch.tensor, \n",
    "        target: torch.tensor,\n",
    "        mask: torch.tensor,\n",
    "        na_pad: torch.tensor,\n",
    "        ) -> torch.tensor:\n",
    "    \"\"\"custome MSE loss to mask padding & nan values\n",
    "    :param input: original vector\n",
    "    :param target: target vector\n",
    "    :param \n",
    "    \"\"\"\n",
    "    loss = nn.MSELoss()\n",
    "    dims = input.shape[-1]\n",
    "    na_mask = input == na_pad\n",
    "    expanded_mask = expand_mask(mask, dims)\n",
    "    new_mask = na_mask.astype(torch.bool) + expanded_mask.astype(torch.bool)\n",
    "    masked_input = torch.masked_select(input, ~new_mask) # mask itself is True if masked\n",
    "    masked_target = torch.masked_select(target, ~new_mask)\n",
    "    return loss(masked_input, masked_target)\n",
    "\n",
    "def composite_mseloss(mse_losses: Sequence[torch.tensor]):\n",
    "    mean_loss = torch.mean(mse_losses)\n",
    "    penalty_loss = torch.sum([(loss - mean_loss)**2 for loss in mse_losses])\n",
    "    composite_loss = mean_loss + penalty_loss\n",
    "    return composite_loss\n",
    "\n",
    "def multiple_input_masked_mse_loss(\n",
    "        inputs: Sequence[torch.tensor],\n",
    "        targets: Sequence[torch.tensor],\n",
    "        masks: Sequence[torch.tensor],\n",
    "        na_pads: Sequence[torch.tensor]):\n",
    "    losses = []\n",
    "    for input, target, mask, na_pad in zip(\n",
    "        inputs, targets, masks, na_pads):\n",
    "        loss = masked_mse_loss(input, target, mask, na_pad)\n",
    "        losses.append(loss)\n",
    "    composite_loss = composite_mseloss(losses)\n",
    "    return composite_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): BatchNorm1d(170, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=170, out_features=42, bias=True)\n",
      "  (3): Linear(in_features=42, out_features=10, bias=True)\n",
      "  (4): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (5): Linear(in_features=2, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.linear_encoder_layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Optional\n",
    "def encode(\n",
    "        model, \n",
    "        inputs: Sequence[Tuple[torch.tensor, Optional[torch.tensor]]],\n",
    "        padding_masks: Sequence[torch.tensor]\n",
    "        ) -> Tuple[torch.tensor, Tuple[torch.tensor]]:\n",
    "    \"\"\"encode a list of inputs of different lengths and dimensionalities\n",
    "    into a single embedding vector\n",
    "    \"\"\"\n",
    "    embeddings, memories = [], []\n",
    "    for input, mask, transformer_encoder, linear_encoder in zip(\n",
    "        inputs, padding_masks, model.transformer_encoders, model.linear_encoder_layers):\n",
    "        print(input.shape)\n",
    "        x_ = transformer_encoder(input, src_key_padding_mask=mask)\n",
    "        memories.append(x_)\n",
    "        embedded = linear_encoder(x_)\n",
    "        print(embedded.shape)\n",
    "        embeddings.append(embedded)\n",
    "    _embedding = torch.stack(embeddings, dim=-1)\n",
    "    print(_embedding.shape)\n",
    "    embedding = model.linear_encoder(_embedding)\n",
    "    print(embedding.shape)\n",
    "    embedding = model.tanh(embedding)\n",
    "    print(\"finished encoder\\n===\\n\")\n",
    "    return (embedding, memories)\n",
    "\n",
    "\n",
    "def decode(\n",
    "        model, \n",
    "        embedding: torch.tensor, \n",
    "        memories: Sequence[torch.tensor]) -> Sequence[torch.tensor]:\n",
    "    print(\"decoder input shape: \\n\", embedding.shape)\n",
    "    _embeddings = model.linear_decoder(embedding)\n",
    "    print(\"after linear embedding shape:\\n\", _embeddings.shape)\n",
    "    reconstructed_xs = []\n",
    "    for i in range(model.num_inputs):\n",
    "        _output = model.linear_decoder_layers[i](_embeddings[:, :, i])\n",
    "        output = model.transformer_decoders[i](_output, memory=memories[i])\n",
    "        reconstructed_xs.append(output)\n",
    "    return reconstructed_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 17])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3, 1])\n",
      "torch.Size([16, 3])\n",
      "finished encoder\n",
      "===\n",
      "\n",
      "decoder input shape: \n",
      " torch.Size([16, 3])\n",
      "after linear embedding shape:\n",
      " torch.Size([16, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "input, mask = input.to(DEVICE), mask.to(DEVICE)\n",
    "embedding, memories = encode(model, [input], [mask])\n",
    "output = decode(model, embedding, memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/stockencoder_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 1])\n",
      "torch.Size([16, 3])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m embedding, memories \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode([\u001b[38;5;28minput\u001b[39m], [mask])\n\u001b[1;32m     47\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(embedding, memories)\n\u001b[0;32m---> 48\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmasked_mse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m running_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     50\u001b[0m logger\u001b[38;5;241m.\u001b[39madd_scaler(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss/train_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), step\u001b[38;5;241m=\u001b[39mepoch\u001b[38;5;241m*\u001b[39mnum_batches \u001b[38;5;241m+\u001b[39m i)\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mmasked_mse_loss\u001b[0;34m(input, target, mask, na_pad)\u001b[0m\n\u001b[1;32m     14\u001b[0m na_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m==\u001b[39m na_pad\n\u001b[1;32m     15\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m expand_mask(mask, dims)\n\u001b[0;32m---> 16\u001b[0m new_mask \u001b[38;5;241m=\u001b[39m \u001b[43mna_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(torch\u001b[38;5;241m.\u001b[39mbool) \u001b[38;5;241m+\u001b[39m expanded_mask\u001b[38;5;241m.\u001b[39mastype(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     17\u001b[0m masked_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m~\u001b[39mnew_mask) \u001b[38;5;66;03m# mask itself is True if masked\u001b[39;00m\n\u001b[1;32m     18\u001b[0m masked_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(target, \u001b[38;5;241m~\u001b[39mnew_mask)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "from models.autoencoder import BaseAutoEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "RUN = 1\n",
    "LR = 1e-4\n",
    "NUM_TRANSFORMER_LAYERS = 5\n",
    "WINDOW_SIZE = 10\n",
    "NHEADS = 1\n",
    "ENCODING_DIM = 3\n",
    "MAX_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "logger_stem = \"./traininglog/fundamental_encoder/runs/\"\n",
    "logger = SummaryWriter(f\"{logger_stem}run{RUN};lr={LR};notflayrs={NUM_TRANSFORMER_LAYERS};wd={WINDOW_SIZE};nh={NHEADS};edim={ENCODING_DIM};bsize={BATCH_SIZE}\")\n",
    "\n",
    "fundamental_data_loader = DataLoader(\n",
    "    fund_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=BATCH_SIZE\n",
    ")\n",
    "\n",
    "num_batches = len(fundamental_data_loader)\n",
    "\n",
    "model = BaseAutoEncoder(\n",
    "    window_sizes=[WINDOW_SIZE],\n",
    "    encoding_dim=ENCODING_DIM, \n",
    "    num_transformer_layers=[NUM_TRANSFORMER_LAYERS], \n",
    "    dims=[17],\n",
    "    activation_func=F.tanh,\n",
    "    nheads=[NHEADS],\n",
    "    device=DEVICE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LR, betas=[0.9, 0.99], eps=1e-07)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    running_losses = []\n",
    "    for i, (input, mask) in enumerate(fundamental_data_loader):\n",
    "        # forward pass\n",
    "        input, mask = input.to(DEVICE), mask.to(DEVICE)\n",
    "        embedding, memories = model.encode([input], [mask])\n",
    "        output = model.decode(embedding, memories)\n",
    "        loss = masked_mse_loss(input, output, mask, na_pad=DEFAULTS.padding_val)\n",
    "        running_losses.append(loss.item())\n",
    "        logger.add_scaler(\"loss/train_step\", loss.item(), step=epoch*num_batches + i)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = np.mean(running_losses)\n",
    "    logger.add_scalar(\"loss/train\", mean_loss, step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p.peng\\Anaconda3\\envs\\stockencoder_env\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:366: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e+10,  2.2513e-01, -1.0000e+10,  8.9167e-01,  9.5669e-01],\n",
       "        [-1.0000e+10,  2.2591e-01, -1.0000e+10,  8.9041e-01,  9.5619e-01],\n",
       "        [-1.0000e+10,  2.2721e-01, -1.0000e+10,  8.8854e-01,  9.5544e-01],\n",
       "        ...,\n",
       "        [-1.0000e+10,  2.7621e-01, -6.6862e+00,  8.8338e-01,  9.2140e-01],\n",
       "        [-1.0000e+10,  2.7616e-01, -2.5919e+00,  8.8354e-01,  9.2150e-01],\n",
       "        [-1.0000e+10,  2.7332e-01, -1.0000e+10,  8.9210e-01,  9.2715e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data[(1, dt.date(2022, 1, 1))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
